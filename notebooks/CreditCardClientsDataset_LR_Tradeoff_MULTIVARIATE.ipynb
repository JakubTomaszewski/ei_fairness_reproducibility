{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import torch.optim as optim\n",
    "\n",
    "from algorithms.algorithms import *\n",
    "from utils.models import *\n",
    "from utils.dataloaders import *\n",
    "\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "np.random.seed(0)\n",
    "\n",
    "dataset = CreditCardClientsDataset(device=device, sensitive_feature_labels=[\"SEX\", \"AGE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pareto_frontier_multi(myArray):\n",
    "    # Sort on first dimension\n",
    "    myArray = myArray[myArray[:,0].argsort()[::-1]]\n",
    "    # Add first row to pareto_frontier\n",
    "    pareto_frontier = myArray[0:1,:]\n",
    "    # Test next row against the last row in pareto_frontier\n",
    "    for row in myArray[1:,:]:\n",
    "        if row[0]<pareto_frontier[-1][0] and row[1]<pareto_frontier[-1][1]:\n",
    "            # If it is better on all features add the row to pareto_frontier\n",
    "            pareto_frontier = np.concatenate((pareto_frontier, [row]))\n",
    "        if row[0]==pareto_frontier[-1][0] and row[1]<pareto_frontier[-1][1]:\n",
    "            # If it is better on all features add the row to pareto_frontier\n",
    "            pareto_frontier[-1] = row\n",
    "    return pareto_frontier\n",
    "\n",
    "def lr_kde_model_runner(dataset, hp, seeds):\n",
    "    test = {'accuracy':[],\n",
    "            'ei_disparity':[],\n",
    "            'dp_disparity':[],\n",
    "            'eo_disparity':[],\n",
    "            'eodd_disparity':[]}\n",
    "    \n",
    "    train = {'accuracy':[],\n",
    "            'ei_disparity':[],\n",
    "            'dp_disparity':[],\n",
    "            'eo_disparity':[],\n",
    "            'eodd_disparity':[]}\n",
    "    \n",
    "    val = {'accuracy':[],\n",
    "            'ei_disparity':[],\n",
    "            'dp_disparity':[],\n",
    "            'eo_disparity':[],\n",
    "            'eodd_disparity':[]}\n",
    "\n",
    "    def append_res(l,acc,ei,dp,eo,eodd):\n",
    "        l['accuracy'].append(acc)\n",
    "        l['ei_disparity'].append(ei)\n",
    "        l['dp_disparity'].append(dp)\n",
    "        l['eo_disparity'].append(eo)\n",
    "        l['eodd_disparity'].append(eodd)\n",
    "\n",
    "    for i in range(len(seeds)):\n",
    "        print('training seed', seeds[i] ,'started')\n",
    "        random.seed(int(seeds[i]))\n",
    "        np.random.seed(seeds[i])\n",
    "        torch.manual_seed(seeds[i]) \n",
    "\n",
    "        model = logReg(num_features=dataset.XZ_train.shape[1])\n",
    "        model = model.to(device)\n",
    "        \n",
    "        lr = hp['learning_rate']\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "        \n",
    "        results = trainer_kde_fair(\n",
    "            model,\n",
    "            dataset,\n",
    "            optimizer,\n",
    "            device,\n",
    "            n_epochs=hp['n_epochs'],\n",
    "            batch_size=hp['batch_size'], \n",
    "            z_blind=False,\n",
    "            fairness=hp['fairness'], \n",
    "            lambda_=hp['lambda_'], \n",
    "            h=hp['h'], \n",
    "            delta_huber=hp['delta_huber'], \n",
    "            optimal_effort=True, \n",
    "            delta_effort=hp['delta_effort']\n",
    "            )\n",
    "        \n",
    "        append_res(train,results.train_acc_hist[-1],results.train_ei_hist[-1],results.train_dp_hist[-1],results.train_eo_hist[-1],results.train_eodd_hist[-1])\n",
    "        append_res(val,results.val_acc,results.val_ei,results.val_dp,results.val_eo,results.val_eodd)\n",
    "        append_res(test,results.test_acc,results.test_ei,results.test_dp,results.test_eo,results.test_eodd)\n",
    "\n",
    "    def get_res(l):\n",
    "        res = {}\n",
    "        res['accuracy_mean'] = np.mean(l['accuracy'])\n",
    "        res['accuracy_var'] = np.std(l['accuracy'])\n",
    "        res['accuracy_list'] = l['accuracy']\n",
    "        res['ei_mean'] = np.mean(l['ei_disparity'])\n",
    "        res['ei_var'] = np.std(l['ei_disparity'])\n",
    "        res['ei_list'] = l['ei_disparity']\n",
    "        res['dp_mean'] = np.mean(l['dp_disparity'])\n",
    "        res['dp_var'] = np.std(l['dp_disparity'])\n",
    "        res['dp_list'] = l['dp_disparity']\n",
    "        res['eo_mean'] = np.mean(l['eo_disparity'])\n",
    "        res['eo_var'] = np.std(l['eo_disparity'])\n",
    "        res['eo_list'] = l['eo_disparity']\n",
    "        res['eodd_mean'] = np.mean(l['eodd_disparity'])\n",
    "        res['eodd_var'] = np.std(l['eodd_disparity'])\n",
    "        res['eodd_list'] = l['eodd_disparity']\n",
    "        return res\n",
    "\n",
    "    res_train = get_res(train)\n",
    "    res_val = get_res(val)\n",
    "    res_test = get_res(test)\n",
    "    print('Training finished for all seeds.')\n",
    "    \n",
    "    return res_train, res_val, res_test\n",
    "\n",
    "def kde_tradeoff(dataset, hp_test, seeds):\n",
    "    hp = hp_test.copy()\n",
    "    result = []\n",
    "    result_test = []\n",
    "    for i in hp_test['learning_rate']:\n",
    "        for k in hp_test['lambda_']:\n",
    "            for seed in seeds:\n",
    "                c = []\n",
    "                c_test = []\n",
    "                hp['learning_rate'] = i\n",
    "                hp['lambda_'] = k\n",
    "                train, val, test = lr_kde_model_runner(dataset, hp, seeds=[seed])\n",
    "                c.append(hp['learning_rate'])\n",
    "                c.append(hp['lambda_'])\n",
    "                c.append(train['accuracy_mean'])\n",
    "                c.append(val['accuracy_mean'])\n",
    "                c.append(val['ei_mean'])\n",
    "                c.append(val['dp_mean'])\n",
    "                c.append(val['eo_mean'])\n",
    "                c.append(val['eodd_mean'])\n",
    "                c_test.append(test['accuracy_mean'])\n",
    "                c_test.append(test['ei_mean'])\n",
    "                result_test.append(c_test)\n",
    "                result.append(c)\n",
    "    print(tabulate(result, headers=['learning_rate', 'lambda_', 'accuracy_train', 'accuracy_val','ei', 'dp', 'eo', 'eodd']))\n",
    "    return result, result_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EI_hp_test = {}\n",
    "EI_hp_test['lambda_'] = [0]\n",
    "EI_hp_test['n_epochs'] = 100\n",
    "EI_hp_test['batch_size'] = 256\n",
    "EI_hp_test['fairness'] = ''\n",
    "EI_hp_test['h'] = 0.01\n",
    "EI_hp_test['delta_huber'] = 0.5\n",
    "EI_hp_test['delta_effort'] = 1.1\n",
    "EI_hp_test['learning_rate'] = [0.01]\n",
    "EI_hp_test['lambda_'] = np.linspace(0,0.5,20)\n",
    "EI_hp_test['fairness'] = 'EI'\n",
    "seeds= np.arange(4)\n",
    "_, results_kde = kde_tradeoff(dataset, EI_hp_test, seeds)\n",
    "results_kde_pareto = pareto_frontier_multi(np.squeeze(results_kde))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_kde_pareto = pareto_frontier_multi(np.squeeze(results_kde))\n",
    "results_kde_pareto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_fb_model_runner(dataset, hp, seeds):\n",
    "    test = {'accuracy':[],\n",
    "            'ei_disparity':[],\n",
    "            'dp_disparity':[],\n",
    "            'eo_disparity':[],\n",
    "            'eodd_disparity':[]}\n",
    "    \n",
    "    train = {'accuracy':[],\n",
    "            'ei_disparity':[],\n",
    "            'dp_disparity':[],\n",
    "            'eo_disparity':[],\n",
    "            'eodd_disparity':[]}\n",
    "    \n",
    "    val = {'accuracy':[],\n",
    "            'ei_disparity':[],\n",
    "            'dp_disparity':[],\n",
    "            'eo_disparity':[],\n",
    "            'eodd_disparity':[]}\n",
    "\n",
    "    def append_res(l,acc,ei,dp,eo,eodd):\n",
    "        l['accuracy'].append(acc)\n",
    "        l['ei_disparity'].append(ei)\n",
    "        l['dp_disparity'].append(dp)\n",
    "        l['eo_disparity'].append(eo)\n",
    "        l['eodd_disparity'].append(eodd)\n",
    "\n",
    "    for i in range(len(seeds)):\n",
    "        print('training seed', seeds[i] ,'started')\n",
    "        random.seed(int(seeds[i]))\n",
    "        np.random.seed(seeds[i])\n",
    "        torch.manual_seed(seeds[i]) \n",
    "\n",
    "        model = logReg(num_features=dataset.XZ_train.shape[1])\n",
    "        model = model.to(device)\n",
    "        \n",
    "        lr = hp['learning_rate']\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "        \n",
    "        results = trainer_fb_fair(\n",
    "            model,\n",
    "            dataset,\n",
    "            optimizer,\n",
    "            device,\n",
    "            n_epochs=hp['n_epochs'],\n",
    "            batch_size=hp['batch_size'], \n",
    "            z_blind=False,\n",
    "            fairness=hp['fairness'], \n",
    "            lambda_=hp['lambda_'], \n",
    "            optimal_effort=True, \n",
    "            delta_effort=hp['delta_effort']\n",
    "            )\n",
    "        \n",
    "        append_res(train,results.train_acc_hist[-1],results.train_ei_hist[-1],results.train_dp_hist[-1],results.train_eo_hist[-1],results.train_eodd_hist[-1])\n",
    "        append_res(val,results.val_acc,results.val_ei,results.val_dp,results.val_eo,results.val_eodd)\n",
    "        append_res(test,results.test_acc,results.test_ei,results.test_dp,results.test_eo,results.test_eodd)\n",
    "\n",
    "    def get_res(l):\n",
    "        res = {}\n",
    "        res['accuracy_mean'] = np.mean(l['accuracy'])\n",
    "        res['accuracy_var'] = np.std(l['accuracy'])\n",
    "        res['accuracy_list'] = l['accuracy']\n",
    "        res['ei_mean'] = np.mean(l['ei_disparity'])\n",
    "        res['ei_var'] = np.std(l['ei_disparity'])\n",
    "        res['ei_list'] = l['ei_disparity']\n",
    "        res['dp_mean'] = np.mean(l['dp_disparity'])\n",
    "        res['dp_var'] = np.std(l['dp_disparity'])\n",
    "        res['dp_list'] = l['dp_disparity']\n",
    "        res['eo_mean'] = np.mean(l['eo_disparity'])\n",
    "        res['eo_var'] = np.std(l['eo_disparity'])\n",
    "        res['eo_list'] = l['eo_disparity']\n",
    "        res['eodd_mean'] = np.mean(l['eodd_disparity'])\n",
    "        res['eodd_var'] = np.std(l['eodd_disparity'])\n",
    "        res['eodd_list'] = l['eodd_disparity']\n",
    "        return res\n",
    "\n",
    "    res_train = get_res(train)\n",
    "    res_val = get_res(val)\n",
    "    res_test = get_res(test)\n",
    "    print('Training finished for all seeds.')\n",
    "    \n",
    "    return res_train, res_val, res_test\n",
    "\n",
    "\n",
    "def fb_tradeoff(dataset, hp_test, seeds):\n",
    "    hp = hp_test.copy()\n",
    "    result = []\n",
    "    result_test = []\n",
    "    for i in hp_test['learning_rate']:\n",
    "        for k in hp_test['lambda_']:\n",
    "            for seed in seeds:\n",
    "                c = []\n",
    "                c_test = []\n",
    "                hp['learning_rate'] = i\n",
    "                hp['lambda_'] = k\n",
    "                train, val, test = lr_fb_model_runner(dataset, hp, seeds=[seed])\n",
    "                c.append(hp['learning_rate'])\n",
    "                c.append(hp['lambda_'])\n",
    "                c.append(train['accuracy_mean'])\n",
    "                c.append(val['accuracy_mean'])\n",
    "                c.append(val['ei_mean'])\n",
    "                c.append(val['dp_mean'])\n",
    "                c.append(val['eo_mean'])\n",
    "                c.append(val['eodd_mean'])\n",
    "                c_test.append(test['accuracy_mean'])\n",
    "                c_test.append(test['ei_mean'])\n",
    "                result_test.append(c_test)\n",
    "                result.append(c)\n",
    "    print(tabulate(result, headers=['learning_rate', 'lambda_', 'accuracy_train', 'accuracy_val','ei', 'dp', 'eo', 'eodd']))\n",
    "    return result, result_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EI_hp_test = {}\n",
    "EI_hp_test['learning_rate'] = [0.01]\n",
    "EI_hp_test['lambda_'] = [0]\n",
    "EI_hp_test['n_epochs'] = 100\n",
    "EI_hp_test['batch_size'] = 256\n",
    "EI_hp_test['delta_effort'] = 1.1\n",
    "EI_hp_test['lambda_'] = np.linspace(0,0.5,25)\n",
    "EI_hp_test['fairness'] = 'EI'\n",
    "\n",
    "_, results_fb = fb_tradeoff(dataset, EI_hp_test, seeds)\n",
    "results_fb_pareto = pareto_frontier_multi(np.squeeze(results_fb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_fc_model_runner(dataset, hp, seeds):\n",
    "    test = {'accuracy':[],\n",
    "            'ei_disparity':[],\n",
    "            'dp_disparity':[],\n",
    "            'eo_disparity':[],\n",
    "            'eodd_disparity':[]}\n",
    "    \n",
    "    train = {'accuracy':[],\n",
    "            'ei_disparity':[],\n",
    "            'dp_disparity':[],\n",
    "            'eo_disparity':[],\n",
    "            'eodd_disparity':[]}\n",
    "    \n",
    "    val = {'accuracy':[],\n",
    "            'ei_disparity':[],\n",
    "            'dp_disparity':[],\n",
    "            'eo_disparity':[],\n",
    "            'eodd_disparity':[]}\n",
    "\n",
    "    def append_res(l,acc,ei,dp,eo,eodd):\n",
    "        l['accuracy'].append(acc)\n",
    "        l['ei_disparity'].append(ei)\n",
    "        l['dp_disparity'].append(dp)\n",
    "        l['eo_disparity'].append(eo)\n",
    "        l['eodd_disparity'].append(eodd)\n",
    "\n",
    "    for i in range(len(seeds)):\n",
    "        print('training seed', seeds[i] ,'started')\n",
    "        random.seed(int(seeds[i]))\n",
    "        np.random.seed(seeds[i])\n",
    "        torch.manual_seed(seeds[i]) \n",
    "\n",
    "        model = logReg(num_features=dataset.XZ_train.shape[1])\n",
    "        model = model.to(device)\n",
    "        \n",
    "        lr = hp['learning_rate']\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "        \n",
    "        results = trainer_fc_fair(\n",
    "            model,\n",
    "            dataset,\n",
    "            optimizer,\n",
    "            device,\n",
    "            n_epochs=hp['n_epochs'],\n",
    "            batch_size=hp['batch_size'], \n",
    "            z_blind=False,\n",
    "            fairness=hp['fairness'], \n",
    "            lambda_=hp['lambda_'],\n",
    "            optimal_effort=True, \n",
    "            delta_effort=hp['delta_effort']\n",
    "            )\n",
    "        append_res(train,results.train_acc_hist[-1],results.train_ei_hist[-1],results.train_dp_hist[-1],results.train_eo_hist[-1],results.train_eodd_hist[-1])\n",
    "        append_res(val,results.val_acc,results.val_ei,results.val_dp,results.val_eo,results.val_eodd)\n",
    "        append_res(test,results.test_acc,results.test_ei,results.test_dp,results.test_eo,results.test_eodd)\n",
    "\n",
    "    def get_res(l):\n",
    "        res = {}\n",
    "        res['accuracy_mean'] = np.mean(l['accuracy'])\n",
    "        res['accuracy_var'] = np.std(l['accuracy'])\n",
    "        res['accuracy_list'] = l['accuracy']\n",
    "        res['ei_mean'] = np.mean(l['ei_disparity'])\n",
    "        res['ei_var'] = np.std(l['ei_disparity'])\n",
    "        res['ei_list'] = l['ei_disparity']\n",
    "        res['dp_mean'] = np.mean(l['dp_disparity'])\n",
    "        res['dp_var'] = np.std(l['dp_disparity'])\n",
    "        res['dp_list'] = l['dp_disparity']\n",
    "        res['eo_mean'] = np.mean(l['eo_disparity'])\n",
    "        res['eo_var'] = np.std(l['eo_disparity'])\n",
    "        res['eo_list'] = l['eo_disparity']\n",
    "        res['eodd_mean'] = np.mean(l['eodd_disparity'])\n",
    "        res['eodd_var'] = np.std(l['eodd_disparity'])\n",
    "        res['eodd_list'] = l['eodd_disparity']\n",
    "        return res\n",
    "\n",
    "    res_train = get_res(train)\n",
    "    res_val = get_res(val)\n",
    "    res_test = get_res(test)\n",
    "    print('Training finished for all seeds.')\n",
    "    \n",
    "    return res_train, res_val, res_test\n",
    "\n",
    "def fc_tradeoff(dataset, hp_test, seeds):\n",
    "    hp = hp_test.copy()\n",
    "    result = []\n",
    "    result_test = []\n",
    "    for i in hp_test['learning_rate']:\n",
    "        for k in hp_test['lambda_']:\n",
    "            for seed in seeds:\n",
    "                c = []\n",
    "                c_test = []\n",
    "                hp['learning_rate'] = i\n",
    "                hp['lambda_'] = k\n",
    "                train, val, test = lr_fc_model_runner(dataset, hp, seeds=[seed])\n",
    "                c.append(hp['learning_rate'])\n",
    "                c.append(hp['lambda_'])\n",
    "                c.append(train['accuracy_mean'])\n",
    "                c.append(val['accuracy_mean'])\n",
    "                c.append(val['ei_mean'])\n",
    "                c.append(val['dp_mean'])\n",
    "                c.append(val['eo_mean'])\n",
    "                c.append(val['eodd_mean'])\n",
    "                c_test.append(test['accuracy_mean'])\n",
    "                c_test.append(test['ei_mean'])\n",
    "                result_test.append(c_test)\n",
    "                result.append(c)\n",
    "    print(tabulate(result, headers=['learning_rate', 'lambda_', 'accuracy_train', 'accuracy_val','ei', 'dp', 'eo', 'eodd']))\n",
    "    return result, result_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EI_hp_test = {}\n",
    "EI_hp_test['learning_rate'] = [0.01]\n",
    "EI_hp_test['lambda_'] = [0]\n",
    "EI_hp_test['n_epochs'] = 100\n",
    "EI_hp_test['batch_size'] = 256\n",
    "EI_hp_test['delta_effort'] = 1.1\n",
    "EI_hp_test['lambda_'] = np.linspace(0,1,100)\n",
    "EI_hp_test['fairness'] = 'EI'\n",
    "\n",
    "_, results_fc = fc_tradeoff(dataset, EI_hp_test, seeds=[0])\n",
    "results_fc_pareto = pareto_frontier_multi(np.squeeze(results_fc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_fc_pareto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('results/german_tradeoff_fb_ei.npy',results_fb_pareto)\n",
    "np.save('results/german_tradeoff_fc_ei.npy',results_fc_pareto)\n",
    "np.save('results/german_tradeoff_kde_ei.npy',results_kde_pareto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FACT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
