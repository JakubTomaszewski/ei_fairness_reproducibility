{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import torch.optim as optim\n",
    "\n",
    "from algorithms.algorithms_jakub import *\n",
    "from utils.models import *\n",
    "from utils.dataloaders import *\n",
    "\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 1], [0, 1]]\n"
     ]
    }
   ],
   "source": [
    "device = 'cpu'\n",
    "dataset = GermanDataset(device=device, sensitive_feature_labels=[\"Age\", \"Sex\"])\n",
    "\n",
    "print(dataset.sensitive_attrs)\n",
    "#print(dataset.XZ_train[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_res():\n",
    "    test = {'accuracy':[],\n",
    "            'ei_disparity':[],\n",
    "            'be_disparity':[],\n",
    "            'dp_disparity':[],\n",
    "            'eo_disparity':[],\n",
    "            'eodd_disparity':[]}\n",
    "    \n",
    "    train = {'accuracy':[],\n",
    "            'ei_disparity':[],\n",
    "            'be_disparity':[],\n",
    "            'dp_disparity':[],\n",
    "            'eo_disparity':[],\n",
    "            'eodd_disparity':[]}\n",
    "    \n",
    "    val = {'accuracy':[],\n",
    "            'ei_disparity':[],\n",
    "            'be_disparity':[],\n",
    "            'dp_disparity':[],\n",
    "            'eo_disparity':[],\n",
    "            'eodd_disparity':[]}\n",
    "    return train, val, test\n",
    "\n",
    "def append_res(l,acc,ei,be,dp,eo,eodd):\n",
    "    l['accuracy'].append(acc)\n",
    "    l['ei_disparity'].append(ei)\n",
    "    l['be_disparity'].append(be)\n",
    "    l['dp_disparity'].append(dp)\n",
    "    l['eo_disparity'].append(eo)\n",
    "    l['eodd_disparity'].append(eodd)\n",
    "\n",
    "def get_res(l):\n",
    "    res = {}\n",
    "    res['accuracy_mean'] = np.mean(l['accuracy'])\n",
    "    res['accuracy_var'] = np.std(l['accuracy'])\n",
    "    res['accuracy_list'] = l['accuracy']\n",
    "    res['ei_mean'] = np.mean(l['ei_disparity'])\n",
    "    res['ei_var'] = np.std(l['ei_disparity'])\n",
    "    res['ei_list'] = l['ei_disparity']\n",
    "    res['be_mean'] = np.mean(l['be_disparity'])\n",
    "    res['be_var'] = np.std(l['be_disparity'])\n",
    "    res['be_list'] = l['be_disparity']\n",
    "    res['dp_mean'] = np.mean(l['dp_disparity'])\n",
    "    res['dp_var'] = np.std(l['dp_disparity'])\n",
    "    res['dp_list'] = l['dp_disparity']\n",
    "    res['eo_mean'] = np.mean(l['eo_disparity'])\n",
    "    res['eo_var'] = np.std(l['eo_disparity'])\n",
    "    res['eo_list'] = l['eo_disparity']\n",
    "    res['eodd_mean'] = np.mean(l['eodd_disparity'])\n",
    "    res['eodd_var'] = np.std(l['eodd_disparity'])\n",
    "    res['eodd_list'] = l['eodd_disparity']\n",
    "    return res\n",
    "\n",
    "def lr_kde_model_runner(dataset, hp, seeds):\n",
    "    \n",
    "    train, val, test = generate_res()\n",
    "\n",
    "    for i in range(len(seeds)):\n",
    "        print('training seed', seeds[i] ,'started')\n",
    "        random.seed(seeds[i])\n",
    "        np.random.seed(seeds[i])\n",
    "        torch.manual_seed(seeds[i]) \n",
    "\n",
    "        model = logReg(num_features=dataset.XZ_train.shape[1])\n",
    "        model = model.to(device)\n",
    "        \n",
    "        lr = hp['learning_rate']\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "        \n",
    "        results = trainer_kde_fair(\n",
    "            model,\n",
    "            dataset,\n",
    "            optimizer,\n",
    "            device,\n",
    "            n_epochs=hp['n_epochs'],\n",
    "            batch_size=hp['batch_size'], \n",
    "            z_blind=False,\n",
    "            fairness=hp['fairness'], \n",
    "            lambda_=hp['lambda_'], \n",
    "            h=hp['h'], \n",
    "            delta_huber=hp['delta_huber'], \n",
    "            optimal_effort=True, \n",
    "            delta_effort=hp['delta_effort']\n",
    "            )\n",
    "        \n",
    "        append_res(train,results.train_acc_hist[-1],results.train_ei_hist[-1],results.train_be_hist[-1],results.train_dp_hist[-1],results.train_eo_hist[-1],results.train_eodd_hist[-1])\n",
    "        append_res(val,results.val_acc,results.val_ei,results.val_be,results.val_dp,results.val_eo,results.val_eodd)\n",
    "        append_res(test,results.test_acc,results.test_ei,results.test_be,results.test_dp,results.test_eo,results.test_eodd)\n",
    "\n",
    "    res_train = get_res(train)\n",
    "    res_val = get_res(val)\n",
    "    res_test = get_res(test)\n",
    "    print('Training finished for all seeds.')\n",
    "    \n",
    "    return res_train, res_val, res_test\n",
    "\n",
    "def lr_fb_model_runner(dataset, hp, seeds):\n",
    "\n",
    "    train, val, test = generate_res()\n",
    "\n",
    "    for i in range(len(seeds)):\n",
    "        print('training seed', seeds[i] ,'started')\n",
    "        random.seed(seeds[i])\n",
    "        np.random.seed(seeds[i])\n",
    "        torch.manual_seed(seeds[i]) \n",
    "\n",
    "        model = logReg(num_features=dataset.XZ_train.shape[1])\n",
    "        model = model.to(device)\n",
    "        \n",
    "        lr = hp['learning_rate']\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "        \n",
    "        results = trainer_fb_fair(\n",
    "            model,\n",
    "            dataset,\n",
    "            optimizer,\n",
    "            device,\n",
    "            n_epochs=hp['n_epochs'],\n",
    "            batch_size=hp['batch_size'], \n",
    "            z_blind=False,\n",
    "            fairness=hp['fairness'], \n",
    "            lambda_=hp['lambda_'],\n",
    "            optimal_effort=True, \n",
    "            delta_effort=hp['delta_effort']\n",
    "            )\n",
    "        \n",
    "        append_res(train,results.train_acc_hist[-1],results.train_ei_hist[-1],results.train_be_hist[-1],results.train_dp_hist[-1],results.train_eo_hist[-1],results.train_eodd_hist[-1])\n",
    "        append_res(val,results.val_acc,results.val_ei,results.val_be,results.val_dp,results.val_eo,results.val_eodd)\n",
    "        append_res(test,results.test_acc,results.test_ei,results.test_be,results.test_dp,results.test_eo,results.test_eodd)\n",
    "\n",
    "    res_train = get_res(train)\n",
    "    res_val = get_res(val)\n",
    "    res_test = get_res(test)\n",
    "    print('Training finished for all seeds.')\n",
    "    \n",
    "    return res_train, res_val, res_test\n",
    "\n",
    "def lr_fc_model_runner(dataset, hp, seeds):\n",
    "    \n",
    "    train, val, test = generate_res()\n",
    "\n",
    "    for i in range(len(seeds)):\n",
    "        print('training seed', seeds[i] ,'started')\n",
    "        random.seed(seeds[i])\n",
    "        np.random.seed(seeds[i])\n",
    "        torch.manual_seed(seeds[i]) \n",
    "\n",
    "        model = logReg(num_features=dataset.XZ_train.shape[1])\n",
    "        model = model.to(device)\n",
    "        \n",
    "        lr = hp['learning_rate']\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "        \n",
    "        results = trainer_fc_fair(\n",
    "            model,\n",
    "            dataset,\n",
    "            optimizer,\n",
    "            device,\n",
    "            n_epochs=hp['n_epochs'],\n",
    "            batch_size=hp['batch_size'], \n",
    "            z_blind=False,\n",
    "            fairness=hp['fairness'], \n",
    "            lambda_=hp['lambda_'],\n",
    "            optimal_effort=True, \n",
    "            delta_effort=hp['delta_effort']\n",
    "            )\n",
    "        \n",
    "        append_res(train,results.train_acc_hist[-1],results.train_ei_hist[-1],results.train_be_hist[-1],results.train_dp_hist[-1],results.train_eo_hist[-1],results.train_eodd_hist[-1])\n",
    "        append_res(val,results.val_acc,results.val_ei,results.val_be,results.val_dp,results.val_eo,results.val_eodd)\n",
    "        append_res(test,results.test_acc,results.test_ei,results.test_be,results.test_dp,results.test_eo,results.test_eodd)\n",
    "\n",
    "    res_train = get_res(train)\n",
    "    res_val = get_res(val)\n",
    "    res_test = get_res(test)\n",
    "    print('Training finished for all seeds.')\n",
    "    \n",
    "    return res_train, res_val, res_test\n",
    "\n",
    "def experiment_runner(dataset, SGD_hp, EI_hp_fc, EI_hp_kde, EI_hp_fb, BE_hp_fb, seeds):\n",
    "    \n",
    "    _, _, SGD = lr_kde_model_runner(dataset, SGD_hp, seeds)\n",
    "    _, _, EI_fc = lr_fc_model_runner(dataset, EI_hp_fc, seeds)\n",
    "    _, _, EI_kde = lr_kde_model_runner(dataset, EI_hp_kde, seeds)\n",
    "    _, _, BE_fb = lr_fb_model_runner(dataset, BE_hp_fb, seeds)\n",
    "    _, _, EI_fb = lr_fb_model_runner(dataset, EI_hp_fb, seeds)\n",
    "    \n",
    "    return SGD, EI_fc, EI_kde, EI_fb, BE_fb\n",
    "\n",
    "def fb_hyperparameter_test(dataset, hp_test, seed=0):\n",
    "    hp = hp_test.copy()\n",
    "    result = []\n",
    "    for i in hp_test['learning_rate']:\n",
    "        for k in hp_test['lambda_']:\n",
    "            c = []\n",
    "            hp['learning_rate'] = i\n",
    "            hp['lambda_'] = k\n",
    "            train, val, _ = lr_fb_model_runner(dataset, hp, seeds=[seed])\n",
    "            c.append(hp['learning_rate'])\n",
    "            c.append(hp['lambda_'])\n",
    "            c.append(train['accuracy_mean'])\n",
    "            c.append(val['accuracy_mean'])\n",
    "            c.append(val['ei_mean'])\n",
    "            c.append(val['be_mean'])\n",
    "            c.append(val['dp_mean'])\n",
    "            c.append(val['eo_mean'])\n",
    "            c.append(val['eodd_mean'])\n",
    "            result.append(c)\n",
    "    print(tabulate(result, headers=['learning_rate', 'lambda_', 'accuracy_train', 'accuracy_val','ei', 'be', 'dp', 'eo', 'eodd']))\n",
    "\n",
    "def kde_hyperparameter_test(dataset, hp_test, seed=0):\n",
    "    hp = hp_test.copy()\n",
    "    result = []\n",
    "    for i in hp_test['learning_rate']:\n",
    "        for k in hp_test['lambda_']:\n",
    "            c = []\n",
    "            hp['learning_rate'] = i\n",
    "            hp['lambda_'] = k\n",
    "            train, val, _ = lr_kde_model_runner(dataset, hp, seeds=[seed])\n",
    "            c.append(hp['learning_rate'])\n",
    "            c.append(hp['lambda_'])\n",
    "            c.append(train['accuracy_mean'])\n",
    "            c.append(val['accuracy_mean'])\n",
    "            c.append(val['ei_mean'])\n",
    "            c.append(val['be_mean'])\n",
    "            c.append(val['dp_mean'])\n",
    "            c.append(val['eo_mean'])\n",
    "            c.append(val['eodd_mean'])\n",
    "            result.append(c)\n",
    "    print(tabulate(result, headers=['learning_rate', 'lambda_', 'accuracy_train', 'accuracy_val','ei', 'be', 'dp', 'eo', 'eodd']))\n",
    "\n",
    "def fc_hyperparameter_test(dataset, hp_test, seed=0):\n",
    "    hp = hp_test.copy()\n",
    "    result = []\n",
    "    for i in hp_test['learning_rate']:\n",
    "        for k in hp_test['lambda_']:\n",
    "            c = []\n",
    "            hp['learning_rate'] = i\n",
    "            hp['lambda_'] = k\n",
    "            train, val, _ = lr_fc_model_runner(dataset, hp, seeds=[seed])\n",
    "            c.append(hp['learning_rate'])\n",
    "            c.append(hp['lambda_'])\n",
    "            c.append(train['accuracy_mean'])\n",
    "            c.append(val['accuracy_mean'])\n",
    "            c.append(val['ei_mean'])\n",
    "            c.append(val['be_mean'])\n",
    "            c.append(val['dp_mean'])\n",
    "            c.append(val['eo_mean'])\n",
    "            c.append(val['eodd_mean'])\n",
    "            result.append(c)\n",
    "    print(tabulate(result, headers=['learning_rate', 'lambda_', 'accuracy_train', 'accuracy_val','ei', 'be', 'dp', 'eo', 'eodd']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SGD Hyperparameter Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training seed 0 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\s1\\fact\\ei_fairness_reproducibility\\notebooks\\..\\utils\\dataloaders.py:10: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\utils\\tensor_new.cpp:264.)\n",
      "  return torch.FloatTensor(X).to(device), torch.FloatTensor(Y).to(device), torch.FloatTensor(Z).to(device), torch.FloatTensor(XZ).to(device)\n",
      "Training: 100%|██████████| 100/100 [00:03<00:00, 25.24epochs/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished for all seeds.\n",
      "training seed 0 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [00:02<00:00, 39.80epochs/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished for all seeds.\n",
      "training seed 0 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [00:01<00:00, 60.22epochs/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished for all seeds.\n",
      "training seed 0 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [00:02<00:00, 42.91epochs/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished for all seeds.\n",
      "  learning_rate    lambda_    accuracy_train    accuracy_val         ei          be         dp         eo       eodd\n",
      "---------------  ---------  ----------------  --------------  ---------  ----------  ---------  ---------  ---------\n",
      "         0.0001          0          0.317188         0.31875  0.0105909  0.00902778  0.05       0.115044   0.115044\n",
      "         0.001           0          0.673438         0.6875   0.127273   0.00347222  0.0423611  0.0980531  0.0980531\n",
      "         0.01            0          0.75             0.78125  0.125      0.0673611   0.183333   0.106903   0.167021\n",
      "         0.1             0          0.779687         0.75625  0.180392   0.034375    0.209722   0.124956   0.203191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "SGD_hp_test = {}\n",
    "SGD_hp_test['learning_rate'] = [0.0001, 0.001, 0.01, 0.1]\n",
    "SGD_hp_test['lambda_'] = [0]\n",
    "SGD_hp_test['n_epochs'] = 100\n",
    "SGD_hp_test['batch_size'] = 1024\n",
    "SGD_hp_test['fairness'] = ''\n",
    "SGD_hp_test['h'] = 0.01\n",
    "SGD_hp_test['delta_huber'] = 0.5\n",
    "SGD_hp_test['delta_effort'] = 0.5\n",
    "\n",
    "kde_hyperparameter_test(dataset, SGD_hp_test, seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learning rate is decided as 0.005.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SGD_hp = SGD_hp_test.copy()\n",
    "SGD_hp['learning_rate'] = 0.1\n",
    "SGD_hp['lambda_'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BE Hyperparameter Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training seed 0 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/100 [00:00<?, ?epochs/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/100 [43:30<?, ?epochs/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m BE_hp_test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlambda_\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m0.2\u001b[39m, \u001b[38;5;241m0.3\u001b[39m, \u001b[38;5;241m0.4\u001b[39m]\n\u001b[0;32m      4\u001b[0m BE_hp_test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfairness\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBE\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 6\u001b[0m \u001b[43mfb_hyperparameter_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBE_hp_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[4], line 197\u001b[0m, in \u001b[0;36mfb_hyperparameter_test\u001b[1;34m(dataset, hp_test, seed)\u001b[0m\n\u001b[0;32m    195\u001b[0m hp[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m i\n\u001b[0;32m    196\u001b[0m hp[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlambda_\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m k\n\u001b[1;32m--> 197\u001b[0m train, val, _ \u001b[38;5;241m=\u001b[39m \u001b[43mlr_fb_model_runner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    198\u001b[0m c\u001b[38;5;241m.\u001b[39mappend(hp[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m    199\u001b[0m c\u001b[38;5;241m.\u001b[39mappend(hp[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlambda_\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Cell \u001b[1;32mIn[4], line 113\u001b[0m, in \u001b[0;36mlr_fb_model_runner\u001b[1;34m(dataset, hp, seeds)\u001b[0m\n\u001b[0;32m    110\u001b[0m lr \u001b[38;5;241m=\u001b[39m hp[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    111\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlr, weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-4\u001b[39m)\n\u001b[1;32m--> 113\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer_fb_fair\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    114\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    115\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    116\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    117\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhp\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mn_epochs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    119\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhp\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbatch_size\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    120\u001b[0m \u001b[43m    \u001b[49m\u001b[43mz_blind\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    121\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfairness\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhp\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfairness\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlambda_\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhp\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlambda_\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimal_effort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelta_effort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhp\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdelta_effort\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    127\u001b[0m append_res(train,results\u001b[38;5;241m.\u001b[39mtrain_acc_hist[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m],results\u001b[38;5;241m.\u001b[39mtrain_ei_hist[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m],results\u001b[38;5;241m.\u001b[39mtrain_be_hist[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m],results\u001b[38;5;241m.\u001b[39mtrain_dp_hist[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m],results\u001b[38;5;241m.\u001b[39mtrain_eo_hist[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m],results\u001b[38;5;241m.\u001b[39mtrain_eodd_hist[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m    128\u001b[0m append_res(val,results\u001b[38;5;241m.\u001b[39mval_acc,results\u001b[38;5;241m.\u001b[39mval_ei,results\u001b[38;5;241m.\u001b[39mval_be,results\u001b[38;5;241m.\u001b[39mval_dp,results\u001b[38;5;241m.\u001b[39mval_eo,results\u001b[38;5;241m.\u001b[39mval_eodd)\n",
      "File \u001b[1;32mc:\\Users\\User\\s1\\fact\\ei_fairness_reproducibility\\notebooks\\..\\algorithms\\algorithms_jakub.py:292\u001b[0m, in \u001b[0;36mtrainer_fb_fair\u001b[1;34m(model, dataset, optimizer, device, n_epochs, batch_size, z_blind, fairness, lambda_, optimal_effort, delta_effort, effort_iter, effort_lr, effort_norm)\u001b[0m\n\u001b[0;32m    290\u001b[0m                 \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m    291\u001b[0m             loss_z[z] \u001b[38;5;241m=\u001b[39m (z_batch_e[group_idx, sensitive_attr_idx]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m/\u001b[39mz_batch[z_batch[:, sensitive_attr_idx]\u001b[38;5;241m==\u001b[39mz, sensitive_attr_idx]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m*\u001b[39mloss_func(Yhat_max\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)[group_idx], torch\u001b[38;5;241m.\u001b[39mones(group_idx\u001b[38;5;241m.\u001b[39msum(), device\u001b[38;5;241m=\u001b[39mdevice))\n\u001b[1;32m--> 292\u001b[0m             f_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mabs(loss_z[z] \u001b[38;5;241m-\u001b[39m loss_mean)\n\u001b[0;32m    294\u001b[0m cost \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m lambda_\u001b[38;5;241m*\u001b[39mf_loss\n\u001b[0;32m    296\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mc:\\Users\\User\\s1\\fact\\ei_fairness_reproducibility\\notebooks\\..\\algorithms\\algorithms_jakub.py:292\u001b[0m, in \u001b[0;36mtrainer_fb_fair\u001b[1;34m(model, dataset, optimizer, device, n_epochs, batch_size, z_blind, fairness, lambda_, optimal_effort, delta_effort, effort_iter, effort_lr, effort_norm)\u001b[0m\n\u001b[0;32m    290\u001b[0m                 \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m    291\u001b[0m             loss_z[z] \u001b[38;5;241m=\u001b[39m (z_batch_e[group_idx, sensitive_attr_idx]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m/\u001b[39mz_batch[z_batch[:, sensitive_attr_idx]\u001b[38;5;241m==\u001b[39mz, sensitive_attr_idx]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m*\u001b[39mloss_func(Yhat_max\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)[group_idx], torch\u001b[38;5;241m.\u001b[39mones(group_idx\u001b[38;5;241m.\u001b[39msum(), device\u001b[38;5;241m=\u001b[39mdevice))\n\u001b[1;32m--> 292\u001b[0m             f_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mabs(loss_z[z] \u001b[38;5;241m-\u001b[39m loss_mean)\n\u001b[0;32m    294\u001b[0m cost \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m lambda_\u001b[38;5;241m*\u001b[39mf_loss\n\u001b[0;32m    296\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:1457\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:701\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:1152\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:1135\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:312\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.do_wait_suspend\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\FACT\\lib\\site-packages\\debugpy\\_vendored\\pydevd\\pydevd.py:2070\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[1;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[0;32m   2067\u001b[0m             from_this_thread\u001b[38;5;241m.\u001b[39mappend(frame_custom_thread_id)\n\u001b[0;32m   2069\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads_suspended_single_notification\u001b[38;5;241m.\u001b[39mnotify_thread_suspended(thread_id, thread, stop_reason):\n\u001b[1;32m-> 2070\u001b[0m         keep_suspended \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_wait_suspend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuspend_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_this_thread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes_tracker\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2072\u001b[0m frames_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   2074\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keep_suspended:\n\u001b[0;32m   2075\u001b[0m     \u001b[38;5;66;03m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\FACT\\lib\\site-packages\\debugpy\\_vendored\\pydevd\\pydevd.py:2106\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[1;34m(self, thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[0;32m   2103\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_input_hook()\n\u001b[0;32m   2105\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_internal_commands()\n\u001b[1;32m-> 2106\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcancel_async_evaluation(get_current_thread_id(thread), \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mid\u001b[39m(frame)))\n\u001b[0;32m   2110\u001b[0m \u001b[38;5;66;03m# process any stepping instructions\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "BE_hp_test = SGD_hp_test.copy()\n",
    "BE_hp_test['learning_rate'] = [0.1]\n",
    "BE_hp_test['lambda_'] = [0, 0.1, 0.2, 0.3, 0.4]\n",
    "BE_hp_test['fairness'] = 'BE'\n",
    "\n",
    "fb_hyperparameter_test(dataset, BE_hp_test, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BE_hp_fb = BE_hp_test.copy()\n",
    "BE_hp_fb['learning_rate'] = 0.1\n",
    "BE_hp_fb['lambda_'] = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### EI Hyperparameter Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training seed 0 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [00:02<00:00, 37.14epochs/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished for all seeds.\n",
      "training seed 0 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [00:02<00:00, 38.71epochs/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished for all seeds.\n",
      "training seed 0 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [00:05<00:00, 17.74epochs/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished for all seeds.\n",
      "training seed 0 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [00:02<00:00, 39.11epochs/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished for all seeds.\n",
      "training seed 0 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [00:02<00:00, 47.32epochs/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished for all seeds.\n",
      "training seed 0 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [00:04<00:00, 24.97epochs/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished for all seeds.\n",
      "  learning_rate    lambda_    accuracy_train    accuracy_val         ei         be        dp         eo      eodd\n",
      "---------------  ---------  ----------------  --------------  ---------  ---------  --------  ---------  --------\n",
      "            0.1       0             0.779687         0.75625  0.180392   0.034375   0.209722  0.124956   0.203191\n",
      "            0.1       0.2           0.773438         0.74375  0.165865   0.034375   0.177778  0.124956   0.145745\n",
      "            0.1       0.6           0.765625         0.7375   0.0960591  0.0104167  0.129861  0.0938053  0.138298\n",
      "            0.1       0.8           0.746875         0.74375  0.0409091  0.021875   0.213889  0.151858   0.168085\n",
      "            0.1       0.9           0.460938         0.50625  0.0723684  0.05625    0.213889  0.282124   0.282124\n",
      "            0.1       0.95          0.7              0.70625  0          0          0         0          0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "EI_hp_test = SGD_hp_test.copy()\n",
    "EI_hp_test['learning_rate'] = [0.1]\n",
    "EI_hp_test['lambda_'] = [0, 0.2, 0.6, 0.8, 0.9, 0.95]\n",
    "EI_hp_test['fairness'] = 'EI'\n",
    "\n",
    "kde_hyperparameter_test(dataset, EI_hp_test, seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lambda for EI is decided as 0.9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EI_hp_kde = EI_hp_test.copy()\n",
    "EI_hp_kde['learning_rate'] = 0.1\n",
    "EI_hp_kde['lambda_'] = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training seed 0 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [00:02<00:00, 41.69epochs/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished for all seeds.\n",
      "training seed 0 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [00:04<00:00, 21.07epochs/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished for all seeds.\n",
      "training seed 0 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [00:02<00:00, 33.69epochs/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished for all seeds.\n",
      "training seed 0 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [00:02<00:00, 46.58epochs/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished for all seeds.\n",
      "training seed 0 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [00:03<00:00, 29.62epochs/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished for all seeds.\n",
      "training seed 0 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [00:04<00:00, 21.60epochs/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished for all seeds.\n",
      "  learning_rate    lambda_    accuracy_train    accuracy_val         ei         be        dp        eo      eodd\n",
      "---------------  ---------  ----------------  --------------  ---------  ---------  --------  --------  --------\n",
      "            0.1       0             0.779687         0.75625  0.180392   0.034375   0.209722  0.124956  0.203191\n",
      "            0.1       0.6           0.775            0.75625  0.143137   0.040625   0.209722  0.124956  0.203191\n",
      "            0.1       0.8           0.773438         0.75     0.19697    0.046875   0.19375   0.124956  0.174468\n",
      "            0.1       0.9           0.770312         0.7375   0.268398   0.0625     0.19375   0.156106  0.156106\n",
      "            0.1       0.95          0.771875         0.73125  0.257212   0.053125   0.177778  0.156106  0.156106\n",
      "            0.1       0.99          0.76875          0.725    0.0295567  0.0131944  0.129861  0.124956  0.159574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "EI_hp_test = SGD_hp_test.copy()\n",
    "EI_hp_test['learning_rate'] = [0.1]\n",
    "EI_hp_test['lambda_'] = [0, 0.6, 0.8, 0.9, 0.95, 0.99]\n",
    "EI_hp_test['fairness'] = 'EI'\n",
    "\n",
    "fc_hyperparameter_test(dataset, EI_hp_test, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EI_hp_fc = EI_hp_test.copy()\n",
    "EI_hp_fc['learning_rate'] = 0.1\n",
    "EI_hp_fc['lambda_'] = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training seed 0 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   3%|▎         | 3/100 [00:00<00:03, 25.52epochs/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [00:02<00:00, 48.79epochs/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished for all seeds.\n",
      "training seed 0 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [00:01<00:00, 53.08epochs/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished for all seeds.\n",
      "training seed 0 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [00:04<00:00, 23.18epochs/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished for all seeds.\n",
      "training seed 0 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [00:03<00:00, 30.40epochs/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished for all seeds.\n",
      "training seed 0 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [00:01<00:00, 54.04epochs/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished for all seeds.\n",
      "  learning_rate    lambda_    accuracy_train    accuracy_val         ei         be        dp         eo       eodd\n",
      "---------------  ---------  ----------------  --------------  ---------  ---------  --------  ---------  ---------\n",
      "            0.1        0            0.779687         0.75625  0.180392   0.034375   0.209722  0.124956   0.203191\n",
      "            0.1        0.1          0.775            0.74375  0.183333   0.0361111  0.168056  0.133805   0.133805\n",
      "            0.1        0.2          0.765625         0.75625  0.0882353  0.0458333  0.143056  0.0849558  0.103191\n",
      "            0.1        0.4          0.746875         0.74375  0.166667   0.0583333  0.109722  0.106903   0.144681\n",
      "            0.1        0.6          0.707812         0.70625  0.0555556  0.01875    0.0125    0.025592   0.0414894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "EI_hp_test = SGD_hp_test.copy()\n",
    "EI_hp_test['learning_rate'] = [0.1]\n",
    "EI_hp_test['lambda_'] = [0, 0.1, 0.2, 0.4, 0.6]\n",
    "EI_hp_test['fairness'] = 'EI'\n",
    "\n",
    "fb_hyperparameter_test(dataset, EI_hp_test, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EI_hp_fb = EI_hp_test.copy()\n",
    "EI_hp_fb['learning_rate'] = 0.1\n",
    "EI_hp_fb['lambda_'] = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_12108\\392662720.py:60: DeprecationWarning: Seeding based on hashing is deprecated\n",
      "since Python 3.9 and will be removed in a subsequent version. The only \n",
      "supported seed types are: None, int, float, str, bytes, and bytearray.\n",
      "  random.seed(seeds[i])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training seed 1 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [00:02<00:00, 40.79epochs/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training seed 2 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [00:04<00:00, 24.56epochs/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training seed 3 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [00:02<00:00, 33.57epochs/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training seed 4 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [00:01<00:00, 60.30epochs/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training seed 5 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [00:02<00:00, 39.67epochs/s]\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_12108\\392662720.py:144: DeprecationWarning: Seeding based on hashing is deprecated\n",
      "since Python 3.9 and will be removed in a subsequent version. The only \n",
      "supported seed types are: None, int, float, str, bytes, and bytearray.\n",
      "  random.seed(seeds[i])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished for all seeds.\n",
      "training seed 1 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [00:05<00:00, 19.67epochs/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training seed 2 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [00:02<00:00, 37.13epochs/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training seed 3 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [00:02<00:00, 40.06epochs/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training seed 4 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [00:04<00:00, 22.02epochs/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training seed 5 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [00:02<00:00, 33.64epochs/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished for all seeds.\n",
      "training seed 1 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [00:02<00:00, 35.07epochs/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training seed 2 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [00:05<00:00, 18.88epochs/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training seed 3 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [00:03<00:00, 25.98epochs/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training seed 4 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [00:02<00:00, 35.18epochs/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training seed 5 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [00:06<00:00, 15.66epochs/s]\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_12108\\392662720.py:103: DeprecationWarning: Seeding based on hashing is deprecated\n",
      "since Python 3.9 and will be removed in a subsequent version. The only \n",
      "supported seed types are: None, int, float, str, bytes, and bytearray.\n",
      "  random.seed(seeds[i])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished for all seeds.\n",
      "training seed 1 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [00:02<00:00, 38.23epochs/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training seed 2 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [00:05<00:00, 19.23epochs/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training seed 3 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [00:05<00:00, 18.44epochs/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training seed 4 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [00:03<00:00, 32.49epochs/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training seed 5 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [00:05<00:00, 17.81epochs/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished for all seeds.\n",
      "training seed 1 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [00:03<00:00, 27.93epochs/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training seed 2 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [00:04<00:00, 23.24epochs/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training seed 3 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [00:05<00:00, 17.27epochs/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training seed 4 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [00:03<00:00, 33.09epochs/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training seed 5 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [00:04<00:00, 24.72epochs/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished for all seeds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "seeds = np.arange(1,6)\n",
    "\n",
    "SGD, EI_fc, EI_kde, EI_fb, BE_fb = experiment_runner(dataset, SGD_hp, EI_hp_fc, EI_hp_kde, EI_hp_fb, BE_hp_fb ,seeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model      accuracy_mean    accuracy_var    ei_mean      ei_var    be_mean     be_var    dp_mean      dp_var    eo_mean      eo_var    eodd_mean    eodd_var\n",
      "-------  ---------------  --------------  ---------  ----------  ---------  ---------  ---------  ----------  ---------  ----------  -----------  ----------\n",
      "SGD                0.776      0.00583095  0.033636   0.00455034  0.105818   0.0190896  0.107227   0.0190269   0.109463   0.0151374     0.128242    0.0267759\n",
      "EI FC              0.786      0.0152971   0.0633268  0.0281924   0.0747609  0.0141063  0.0682884  0.008949    0.0799483  0.0234947     0.083551    0.0166657\n",
      "EI KDE             0.722      0.0218174   0.0620284  0.0231114   0.0739394  0.0290602  0.0721358  0.0310583   0.0912719  0.0380076     0.0936493   0.0344409\n",
      "EI FB              0.784      0.0115758   0.0291196  0.0138806   0.0642884  0.0082529  0.0631504  0.0107851   0.0799483  0.0109684     0.0799483   0.0109684\n",
      "BE FB              0.786      0.00969536  0.0415201  0.00773254  0.0259338  0.0113565  0.0232177  0.00591232  0.040324   0.00888534    0.0925025   0.0476193\n"
     ]
    }
   ],
   "source": [
    "result = []\n",
    "models = [\"SGD\", \"EI FC\", \"EI KDE\", \"EI FB\", \"BE FB\"]\n",
    "sol = [SGD, EI_fc, EI_kde, EI_fb, BE_fb]\n",
    "for i in range(len(models)):\n",
    "    c = []\n",
    "    c.append(models[i])\n",
    "    res = sol[i]\n",
    "    c.append(res['accuracy_mean'])\n",
    "    c.append(res['accuracy_var'])\n",
    "    c.append(res['ei_mean'])\n",
    "    c.append(res['ei_var'])\n",
    "    c.append(res['be_mean'])\n",
    "    c.append(res['be_var'])\n",
    "    c.append(res['dp_mean'])\n",
    "    c.append(res['dp_var'])\n",
    "    c.append(res['eo_mean'])\n",
    "    c.append(res['eo_var'])\n",
    "    c.append(res['eodd_mean'])\n",
    "    c.append(res['eodd_var'])\n",
    "    result.append(c)\n",
    "\n",
    "print(tabulate(result, headers=[\"model\",\"accuracy_mean\",\"accuracy_var\",\"ei_mean\",\"ei_var\",\"be_mean\",\"be_var\", \"dp_mean\", \"dp_var\",\"eo_mean\",\"eo_var\",\"eodd_mean\",\"eodd_var\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('sampler_fairness')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "902159ac64d6098f1cb3487d51e5a4224d4aca1118329737e4cb7cda9bc70f7f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
